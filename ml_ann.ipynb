{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-saqsU4fYktvoueskbNWyl7GFRK5Ln4t",
      "authorship_tag": "ABX9TyOqywPq6BlVTQq7U78jagn1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data preprocessing**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3fASKqV0MVDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBl5pU9LqG9K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('bankmkt.csv')"
      ],
      "metadata": {
        "id": "r3QL24Y9q94i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "1idiwN_ErNuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info"
      ],
      "metadata": {
        "id": "qCovQf-DrO0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "fs01rUWCIbXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ZLVInttZIkQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the duplicates\n",
        "dups = df.duplicated()\n",
        "print('before are there any duplicates : ', dups.any())\n",
        "\n",
        "#df.drop_duplicates(inplace=True)\n",
        "# reset indices after dropping rows\n",
        "df=df.reset_index(drop=True)\n",
        "print('after are there any duplicates : ', df.duplicated().any())"
      ],
      "metadata": {
        "id": "SAzn_ut7Kv0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Make a class for easier to experiment**"
      ],
      "metadata": {
        "id": "56Q17hTmMgFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "Ip3Tx610rQp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork:\n",
        "    def __init__(self, dataset_file):\n",
        "        self.dataset_file = dataset_file\n",
        "        self.data = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.model = None\n",
        "\n",
        "    def read_dataset(self):\n",
        "        self.data = pd.read_csv(self.dataset_file)\n",
        "\n",
        "    def preprocess_dataset(self):\n",
        "        selected_features = ['age', 'marital', 'education', 'default', 'balance', 'housing', 'loan',\n",
        "                             'contact', 'campaign', 'pdays', 'previous', 'poutcome']\n",
        "\n",
        "        self.data = self.data.drop(['duration'], axis=1)\n",
        "\n",
        "        # drop the duplicates\n",
        "        dups = self.data.duplicated()\n",
        "        print('before are there any duplicates : ', dups.any())\n",
        "        self.data.drop_duplicates(inplace=True)\n",
        "\n",
        "        # reset indices after dropping rows\n",
        "        self.data=self.data.reset_index(drop=True)\n",
        "        print('after are there any duplicates : ', self.data.duplicated().any())\n",
        "\n",
        "\n",
        "        self.data['housing'] = self.data['housing'].map({'yes': 1, 'no': 0})\n",
        "        self.data['loan'] = self.data['loan'].map({'yes': 1, 'no': 0})\n",
        "        self.data['y'] = self.data['y'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "\n",
        "\n",
        "        target_variable = 'y'\n",
        "        self.X = self.data[selected_features]\n",
        "        self.y = self.data[target_variable]\n",
        "\n",
        "        categorical_features = ['marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
        "        le = LabelEncoder()\n",
        "        for feature in categorical_features:\n",
        "            self.X[feature] = le.fit_transform(self.X[feature])\n",
        "\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "    def build_model(self, hidden_layers, hidden_nodes):\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Dense(hidden_nodes, input_dim=self.X_train.shape[1], activation='relu'))\n",
        "\n",
        "        for _ in range(hidden_layers):\n",
        "            self.model.add(Dense(hidden_nodes, activation='relu'))\n",
        "\n",
        "        self.model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    def train_model(self, epochs, learning_rate):\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        early_stopping = EarlyStopping(patience=10, restore_best_weights=True)  # Early stopping to prevent overfitting\n",
        "\n",
        "        self.model.fit(self.X_train, self.y_train, epochs=epochs, batch_size=32, verbose=1,\n",
        "                       validation_data=(self.X_test, self.y_test), callbacks=[early_stopping])\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        loss, accuracy = self.model.evaluate(self.X_test, self.y_test)\n",
        "        print(f'Test Loss: {loss:.4f}')\n",
        "        print(f'Test Accuracy: {accuracy:.4f}')\n",
        "        return loss, accuracy"
      ],
      "metadata": {
        "id": "yB_yo9kZrUDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kLIuq0grEA4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the feed-forward network\n",
        "network = FeedForwardNetwork('bankmkt.csv')"
      ],
      "metadata": {
        "id": "mmqRRSLlrVLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and preprocess the dataset\n",
        "network.read_dataset()\n",
        "network.preprocess_dataset()"
      ],
      "metadata": {
        "id": "GKLN2ZFiraTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experimental_df=pd.read_csv('experimental.csv')"
      ],
      "metadata": {
        "id": "zF8TsQsUrb1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experimental_df.head(100)"
      ],
      "metadata": {
        "id": "j5BQQ7Eirm39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in experimental_df.iterrows():\n",
        "    print(j['Architecture'])\n",
        "    # Set the network parameters:\n",
        "    hidden_layers = j['HiddenLayers'].astype(int)\n",
        "    hidden_nodes = j['NodesperLayer'].astype(int)\n",
        "    epochs = j['Epochs'].astype(int)\n",
        "    learning_rate = j['LearningRate']\n",
        "\n",
        "    # Build and train the model\n",
        "    network.build_model(hidden_layers, hidden_nodes)\n",
        "    network.train_model(epochs, learning_rate)\n",
        "\n",
        "    # Evaluate the model\n",
        "\n",
        "    experimental_df.at[i,'Loss'], experimental_df.at[i,'Accuracy'] =network.evaluate_model()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZcQ-LVTYropk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experimental_df.head(100)"
      ],
      "metadata": {
        "id": "dTyYAPI-rqXz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}